{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "## General description\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a computational trick to acclerate the optimization for the objective functions of the form\n",
    "$$f(para) = \\sum_{i=1}^m g(data_i; para).$$\n",
    "\n",
    "This optimization problem is computationally hard if $g$ or $g'$ is not easy to compute and $m$ is huge.\n",
    "\n",
    "To solve it, SGD pick a subset $I$ of the data points randomly at each iteration and apply gradient descent to the subset objective function\n",
    "$$f(para) = \\sum_{i \\in I} g(data_i; para).$$\n",
    "\n",
    "## Example: linear regression\n",
    "\n",
    "Suppose we have data generated by $y_i = k \\cdot x_i + N(0,1)$ for $i = 1, \\ldots, m$. We want to find $k$ that minimize the mean square error\n",
    "$$f(k) = \\frac 1m \\sum_{i=1}^m (y_i - k x_i)^2.$$\n",
    "\n",
    "Note we have $f'(k) = -\\frac 2m \\sum_{i=1}^m x_i (y_i - k x_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(1)\n",
    "\n",
    "m = 1000\n",
    "\n",
    "x = collect(1:m) / m * 10\n",
    "y = 3 * x + randn(m);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopping criteria**: we cannot naively apply the stopping criteria of gradient descent to SGD. (why?)\n",
    "\n",
    "We will discuss a proper stopping criteria for SGD later. But now let's use the following stopping rule and assume it can be computed very efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stop_now (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "function stop_now(x, y, k, tol=1e-2)\n",
    "    mean((y - k * x).^2) < mean((y - 3 * x).^2) + tol\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2.99354312434587\n",
      "converges after 6 iterations; call g' 6000 times.\n"
     ]
    }
   ],
   "source": [
    "function gradient_descent(x, y, k0 = 0, α=0.01; tol=1e-3, maxiter=500)\n",
    "    k = k0\n",
    "    iter = 0\n",
    "    \n",
    "    for i = 1:maxiter\n",
    "        gradient = -2 * mean(x .* (y - k * x))\n",
    "        \n",
    "        if stop_now(x, y, k, tol)\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        k -= α*gradient\n",
    "        iter += 1\n",
    "    end\n",
    "    \n",
    "    println(\"k = $(k)\")\n",
    "    return iter\n",
    "end\n",
    "\n",
    "\n",
    "iter_num = gradient_descent(x, y)\n",
    "comp_count = iter_num * m\n",
    "\n",
    "println(\"converges after $(iter_num) iterations; call g' $(comp_count) times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2.9933621095869487\n",
      "converges after 15 iterations; call g' 150 times.\n"
     ]
    }
   ],
   "source": [
    "function stochastic_gradient_descent(x, y, k0 = 0, α=0.01; subset_size = 10, tol=1e-3, maxiter=500)\n",
    "    k = k0\n",
    "    iter = 0\n",
    "    \n",
    "    for i = 1:maxiter\n",
    "        \n",
    "        #generate a subset of data at each iteration\n",
    "        subset = rand(1:length(x), subset_size)\n",
    "        # use the gradient of the subset objective function\n",
    "        gradient = -2 * mean(x[subset] .* (y[subset] - k * x[subset]))\n",
    "        \n",
    "        if stop_now(x, y, k, tol)\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        k -= α*gradient\n",
    "        iter += 1\n",
    "    end\n",
    "    println(\"k = $(k)\")\n",
    "    return iter\n",
    "end\n",
    "\n",
    "\n",
    "Random.seed!(1)\n",
    "\n",
    "subset_size = 10\n",
    "iter_num = stochastic_gradient_descent(x, y, subset_size = subset_size)\n",
    "comp_count = iter_num * subset_size\n",
    "\n",
    "println(\"converges after $(iter_num) iterations; call g' $(comp_count) times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping criteria\n",
    "\n",
    "Let's call the data set above $(x, y)$ training set. Assume we have another data set named validation set that has much small size than the training set but is generated in the same way. e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 10\n",
    "\n",
    "x_val = rand(val_size) * 10\n",
    "y_val = 3 * x_val + randn(val_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set has size much smaller than the training set, so the same $f$ defined on it\n",
    "$$f_{val}(para) = \\sum_{i \\in \\text{validation}} g(data_i; para)$$\n",
    "is easy to compute.  \n",
    "Moreover, since the validation set is generated by the same mechanism, it should represent the overall property of the training set.\n",
    "\n",
    "Combining these two points, we can propose a reasonable and computationally efficient stopping rule named **early stopping**: compute $f_{val}$ at each iteration and stop if we see $f_{val}$ start increasing.\n",
    "\n",
    "![early_stopping](https://i.imgur.com/eP0gppr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some final notes\n",
    "\n",
    "The type of optimization problems that SGD tries to solve is not exactly the same as the traditional optimization problems which minimize $f(x)$. The SGD optimization problem builds its objective function on a data set and **aims to find the mechanism that generate this data set rather than minimizing the objective function itself**. These two goals are similar in many ways but not exactly the same. And this is also why we can introduce a validation data set that is independent of $f(x)$ to help stopping in optimizing $f(x)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
